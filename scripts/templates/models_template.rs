{# 
    Template file for generating the model specification. 
    Use the generate_models.py script to regenerate.
#}
// This file has been automatically generated by Jinja2 via the
// script {{ file }}.
// Please do not change this file by hand.
use crate::{
    activation_functions::{GdnLayer, IgdnLayer, ReluLayer},
    convolutions::{ConvolutionLayer, Padding},
    weight_loader::WeightLoader,
    transposed_convolutions::TransposedConvolutionLayer,
    ImagePrecision,
};
use ndarray::*;

pub type InternalDataRepresentation = Array3<ImagePrecision>;

// A note on the weights:
// Naming convention:
// [architecture]_[coder]_[layer type]_[layer]_[weight type]

/// General model trait for en- and decoding
pub trait CodingModel {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation;
}

impl CodingModel for ConvolutionLayer {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        self.convolve(input)
    }
}

impl CodingModel for TransposedConvolutionLayer {
        fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
            self.transposed_convolve(input)
        }
}
    

impl CodingModel for GdnLayer {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        self.activate(input)
    }
}

impl CodingModel for IgdnLayer {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        self.activate(input)
    }
}

impl CodingModel for ReluLayer {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        self.activate(input)
    }
}

{% for m in models %} 
    pub struct {{m.name}} {
        {% for l in m.layers %}
            layer_{{loop.index0}}: {{l.name}},
            {% if l.activation is not none %}
            activation_{{loop.index0}}: {{l.activation.layer_name}},
            {% endif %}
        {% endfor %}
    }

    impl CodingModel for {{m.name}} {
        fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
            let x = input.clone();
            {% for l in m.layers %}
                let x = self.layer_{{loop.index0}}.forward_pass(&x);
                {% if l.activation is not none %}
                    let x = self.activation_{{loop.index0}}.forward_pass(&x);
                {% endif %}
            {% endfor %}
            x
        }
    }

    impl {{m.name}} {
        pub fn new(loader: &mut impl WeightLoader) -> Self {
            {% for l in m.layers %}
                {% set outer_loop = loop %}
                let layer_{{loop.index0}}_weights = loader.get_weight(
                    "{{m.layer_name}}_{{loop.index0}}/kernel.npy",
                    ({{l.kernel_height}}, {{l.kernel_width}}, {{l.channels}}, {{l.filters}})
                ).unwrap();
                let layer_{{loop.index0}} = {{l.name}}::new_tf(layer_{{loop.index0}}_weights, 
                                                           {{l.stride}}, {{l.padding}});
                {% if l.activation is not none %}
                    {% for w in l.activation.weights %}
                        let activation_{{outer_loop.index0}}_weight_{{loop.index0}} 
                            = loader.get_weight("{{m.weight_name}}/{{m.layer_name}}_{{outer_loop.index0}}/{{l.activation.name}}_{{outer_loop.index0}}/{{w.name}}.npy",
                                                {{w.shape}}).unwrap();
                    {% endfor %}
                    let activation_{{outer_loop.index0}} = {{l.activation.layer_name}}::new(
                        {% for w in l.activation.weights %}
                            activation_{{outer_loop.index0}}_weight_{{loop.index0}},
                        {% endfor %}
                    );
                {% endif %}
            {% endfor %}
            Self {
                {% for l in m.layers %}
                    layer_{{loop.index0}},
                    {% if l.activation is not none %}
                    activation_{{loop.index0}},
                    {% endif %}
                {% endfor %}
            }
        }
    }
{% endfor %}

mod tests {
    use crate::weight_loader::NpzWeightLoader;
    use super::*;

    // Smoke tests cannot be run on Github CI, only designed for manual running,
    // thats why we need the ignore tag
    {% for m in models %}
    #[test]
    fn smoke_test_{{m.name.lower()}}() {
        let mut loader = NpzWeightLoader::full_loader();
        let _encoder = {{m.name}}::new(&mut loader);
    }
    {% endfor %}
}
