{# 
    Template file for generating the model specification. 
    Use the generate_models.py script to regenerate.
#}
//! This module provides the hierarchical models used in the encoding and decoding process.
// This file has been automatically generated by Jinja2 via the
// script {{ file }}.
// Please do not change this file by hand.
use crate::{
    activation_functions::{GdnLayer, IgdnLayer, ReluLayer},
    weight_loader::WeightLoader,
    WeightPrecision,
};
use convolutions_rs::{
    convolutions::ConvolutionLayer, transposed_convolutions::TransposedConvolutionLayer, Padding,
};
use log::trace;
use ndarray::*;

pub type InternalDataRepresentation = Array3<WeightPrecision>;

// A note on the weights:
// Naming convention:
// [architecture]_[coder]_[layer type]_[layer]_[weight type]

/// General model trait for en- and decoding
pub trait CodingModel {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation;
}

impl CodingModel for ConvolutionLayer<WeightPrecision> {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        self.convolve(input)
    }
}

impl CodingModel for TransposedConvolutionLayer<WeightPrecision> {
        fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
            self.transposed_convolve(input)
        }
}
    

impl CodingModel for GdnLayer {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        self.activate(input)
    }
}

impl CodingModel for IgdnLayer {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        self.activate(input)
    }
}

impl CodingModel for ReluLayer {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        self.activate(input)
    }
}

{% for m in models %} 
    pub struct {{m.name}} {
        {% for l in m.layers %}
            layer_{{loop.index0}}: {{l.name}}<WeightPrecision>,
            {% if l.activation is not none %}
            activation_{{loop.index0}}: {{l.activation.layer_name}},
            {% endif %}
        {% endfor %}
    }

    impl CodingModel for {{m.name}} {
        {# Have to allow since the last let might be extraneous due to model generation #}
        #[allow(clippy::let_and_return)]
        fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
            let x = input.clone();
            trace!("input: {:?}\n", x);
            {% for l in m.layers %}
                let x = self.layer_{{loop.index0}}.forward_pass(&x);
                trace!("{{m.layer_name}}_{{loop.index0}}_output: {:?}\n", x);
                {% if l.activation is not none %}
                    let x = self.activation_{{loop.index0}}.forward_pass(&x);
                {% endif %}
                trace!("{{m.layer_name}}_activation_{{loop.index0}}_output: {:?}\n", x);
            {% endfor %}
            x
        }
    }

    impl {{m.name}} {
        pub fn new(loader: &mut impl WeightLoader) -> Self {
            {% for l in m.layers %}
                {% set outer_loop = loop %}
                let layer_{{loop.index0}}_weights = loader.get_weight(
                    "{{m.layer_name}}_{{loop.index0}}/kernel.npy",
                    ({{l.kernel_height}}, {{l.kernel_width}}, {{l.channels}}, {{l.filters}})
                ).unwrap();
                trace!("{{m.layer_name}}_{{loop.index0}}_weight: {:?}\n", layer_{{loop.index0}}_weights);
                let layer_{{loop.index0}} = {{l.name}}::new_tf(layer_{{loop.index0}}_weights, 
                                                           {{l.stride}}, {{l.padding}});
                {% if l.activation is not none %}
                    {% for w in l.activation.weights %}
                        let activation_{{outer_loop.index0}}_weight_{{loop.index0}} 
                            = loader.get_weight("{{m.weight_name}}/{{m.layer_name}}_{{outer_loop.index0}}/{{l.activation.name}}_{{outer_loop.index0}}/{{w.name}}.npy",
                                                {{w.shape}}).unwrap();
                        trace!("{{m.weight_name}}_{{m.layer_name}}_{{l.activation.name}}_{{w.name}}_{{outer_loop.index0}}_weight: {:?}\n", activation_{{outer_loop.index0}}_weight_{{loop.index0}});
                    {% endfor %}
                    let activation_{{outer_loop.index0}} = {{l.activation.layer_name}}::new(
                        {% for w in l.activation.weights %}
                            activation_{{outer_loop.index0}}_weight_{{loop.index0}},
                        {% endfor %}
                    );
                {% endif %}
            {% endfor %}
            Self {
                {% for l in m.layers %}
                    layer_{{loop.index0}},
                    {% if l.activation is not none %}
                    activation_{{loop.index0}},
                    {% endif %}
                {% endfor %}
            }
        }
    }
{% endfor %}

mod tests {
    #[allow(unused_imports)]
    use crate::weight_loader::NpzWeightLoader;
    #[allow(unused_imports)]
    use super::*;

    {% for m in models %}
    #[test]
    fn smoke_test_{{m.name.lower()}}() {
        let mut loader = NpzWeightLoader::full_loader();
        let _encoder = {{m.name}}::new(&mut loader);
    }
    {% endfor %}
}
