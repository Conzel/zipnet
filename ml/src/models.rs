//! This module provides the hierarchical models used in the encoding and decoding process.
// This file has been automatically generated by Jinja2 via the
// script /Users/almico/projects/zipnet/scripts/./generate_models.py.
// Please do not change this file by hand.
use crate::{
    activation_functions::{GdnLayer, IgdnLayer, ReluLayer},
    weight_loader::WeightLoader,
    WeightPrecision,
};
use convolutions_rs::{
    convolutions::ConvolutionLayer, transposed_convolutions::TransposedConvolutionLayer, Padding,
};
use ndarray::*;

pub type InternalDataRepresentation = Array3<WeightPrecision>;

// A note on the weights:
// Naming convention:
// [architecture]_[coder]_[layer type]_[layer]_[weight type]

/// General model trait for en- and decoding
pub trait CodingModel {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation;
}

impl CodingModel for ConvolutionLayer<WeightPrecision> {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        self.convolve(input)
    }
}

impl CodingModel for TransposedConvolutionLayer<WeightPrecision> {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        self.transposed_convolve(input)
    }
}

impl CodingModel for GdnLayer {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        self.activate(input)
    }
}

impl CodingModel for IgdnLayer {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        self.activate(input)
    }
}

impl CodingModel for ReluLayer {
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        self.activate(input)
    }
}

pub struct MinnenEncoder {
    conv0: ConvolutionLayer<WeightPrecision>,

    relu0: ReluLayer,

    conv1: ConvolutionLayer<WeightPrecision>,

    relu1: ReluLayer,

    conv2: ConvolutionLayer<WeightPrecision>,

    relu2: ReluLayer,

    conv3: ConvolutionLayer<WeightPrecision>,
}

impl CodingModel for MinnenEncoder {
    #[allow(clippy::let_and_return)]
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        let x = input.clone();

        let x = self.conv0.forward_pass(&x);

        let x = self.relu0.forward_pass(&x);

        let x = self.conv1.forward_pass(&x);

        let x = self.relu1.forward_pass(&x);

        let x = self.conv2.forward_pass(&x);

        let x = self.relu2.forward_pass(&x);

        let x = self.conv3.forward_pass(&x);

        x
    }
}

impl MinnenEncoder {
    pub fn new(loader: &mut impl WeightLoader) -> Self {
        let conv0_weights = loader
            .get_weight("analysis_transform.conv0.weight.npy", (128, 3, 5, 5))
            .unwrap();

        let conv0 = ConvolutionLayer::new(conv0_weights, 2, Padding::Same);

        let relu0 = ReluLayer::new();

        let conv1_weights = loader
            .get_weight("analysis_transform.conv1.weight.npy", (128, 128, 5, 5))
            .unwrap();

        let conv1 = ConvolutionLayer::new(conv1_weights, 2, Padding::Same);

        let relu1 = ReluLayer::new();

        let conv2_weights = loader
            .get_weight("analysis_transform.conv2.weight.npy", (128, 128, 5, 5))
            .unwrap();

        let conv2 = ConvolutionLayer::new(conv2_weights, 2, Padding::Same);

        let relu2 = ReluLayer::new();

        let conv3_weights = loader
            .get_weight("analysis_transform.conv3.weight.npy", (192, 128, 5, 5))
            .unwrap();

        let conv3 = ConvolutionLayer::new(conv3_weights, 2, Padding::Same);

        Self {
            conv0,

            relu0,

            conv1,

            relu1,

            conv2,

            relu2,

            conv3,
        }
    }
}

pub struct JohnstonDecoder {
    conv_transpose0: TransposedConvolutionLayer<WeightPrecision>,

    relu0: ReluLayer,

    conv_transpose1: TransposedConvolutionLayer<WeightPrecision>,

    relu1: ReluLayer,

    conv_transpose2: TransposedConvolutionLayer<WeightPrecision>,

    relu2: ReluLayer,

    conv_transpose3: TransposedConvolutionLayer<WeightPrecision>,

    relu3: ReluLayer,
}

impl CodingModel for JohnstonDecoder {
    #[allow(clippy::let_and_return)]
    fn forward_pass(&self, input: &InternalDataRepresentation) -> InternalDataRepresentation {
        let x = input.clone();

        let x = self.conv_transpose0.forward_pass(&x);

        let x = self.relu0.forward_pass(&x);

        let x = self.conv_transpose1.forward_pass(&x);

        let x = self.relu1.forward_pass(&x);

        let x = self.conv_transpose2.forward_pass(&x);

        let x = self.relu2.forward_pass(&x);

        let x = self.conv_transpose3.forward_pass(&x);

        let x = self.relu3.forward_pass(&x);

        x
    }
}

impl JohnstonDecoder {
    pub fn new(loader: &mut impl WeightLoader) -> Self {
        let conv_transpose0_weights = loader
            .get_weight(
                "synthesis_transform.conv_transpose0.weight.npy",
                (192, 128, 5, 5),
            )
            .unwrap();

        let conv_transpose0 =
            TransposedConvolutionLayer::new(conv_transpose0_weights, 2, Padding::Same);

        let relu0 = ReluLayer::new();

        let conv_transpose1_weights = loader
            .get_weight(
                "synthesis_transform.conv_transpose1.weight.npy",
                (128, 128, 5, 5),
            )
            .unwrap();

        let conv_transpose1 =
            TransposedConvolutionLayer::new(conv_transpose1_weights, 2, Padding::Same);

        let relu1 = ReluLayer::new();

        let conv_transpose2_weights = loader
            .get_weight(
                "synthesis_transform.conv_transpose2.weight.npy",
                (128, 128, 5, 5),
            )
            .unwrap();

        let conv_transpose2 =
            TransposedConvolutionLayer::new(conv_transpose2_weights, 2, Padding::Same);

        let relu2 = ReluLayer::new();

        let conv_transpose3_weights = loader
            .get_weight(
                "synthesis_transform.conv_transpose3.weight.npy",
                (128, 3, 5, 5),
            )
            .unwrap();

        let conv_transpose3 =
            TransposedConvolutionLayer::new(conv_transpose3_weights, 2, Padding::Same);

        let relu3 = ReluLayer::new();

        Self {
            conv_transpose0,

            relu0,

            conv_transpose1,

            relu1,

            conv_transpose2,

            relu2,

            conv_transpose3,

            relu3,
        }
    }
}

mod tests {
    #[allow(unused_imports)]
    use super::*;
    #[allow(unused_imports)]
    use crate::weight_loader::NpzWeightLoader;

    #[test]
    fn smoke_test_minnenencoder() {
        let mut loader = NpzWeightLoader::full_loader();
        let _encoder = MinnenEncoder::new(&mut loader);
    }

    #[test]
    fn smoke_test_johnstondecoder() {
        let mut loader = NpzWeightLoader::full_loader();
        let _encoder = JohnstonDecoder::new(&mut loader);
    }
}
